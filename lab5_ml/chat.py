import numpy as np
import pickle
import re
from tensorflow.keras.models import load_model
from tensorflow.keras.preprocessing.sequence import pad_sequences


def clean(text: str) -> str:
    """–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è."""
    text = text.lower()
    text = re.sub(r'[^–∞-—è—ëa-z0-9\s.,!?]', ' ', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text


def postprocess_response(text: str) -> str:
    """
    –ß–∏—Å—Ç–∏–º –æ—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏:
    - —É–±–∏—Ä–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã (<start>, <end>, <unk>, –ª—é–±—ã–µ <...>)
    - —É–±–∏—Ä–∞–µ–º –≤–æ–∑–º–æ–∂–Ω–æ–µ 'unk' –∏ 'end'
    - —á–∏—Å—Ç–∏–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã
    """
    # –£–±–∏—Ä–∞–µ–º —è–≤–Ω—ã–µ —Å–ø–µ—Ü—Ç–æ–∫–µ–Ω—ã
    text = text.replace('<start>', ' ')
    text = text.replace('<end>', ' ')
    text = text.replace('<unk>', ' ')

    # –£–±–∏—Ä–∞–µ–º –ª—é–±—ã–µ —à—Ç—É–∫–∏ –≤ —É–≥–ª–æ–≤—ã—Ö —Å–∫–æ–±–∫–∞—Ö —Ç–∏–ø–∞ <—á—Ç–æ_—É–≥–æ–¥–Ω–æ>
    text = re.sub(r'<[^>]+>', ' ', text)

    # –ù–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π —É–±–∏—Ä–∞–µ–º –æ–¥–∏–Ω–æ—á–Ω—ã–µ "unk" –∏ "end"
    text = re.sub(r'\bunk\b', ' ', text, flags=re.IGNORECASE)
    text = re.sub(r'\bend\b', ' ', text, flags=re.IGNORECASE)

    # –ß–∏—Å—Ç–∏–º –ø—Ä–æ–±–µ–ª—ã
    text = re.sub(r'\s+', ' ', text).strip()
    return text


def beam_search(text, encoder, decoder, tokenizer, config, beam_width: int = 5, length_penalty: float = 0.7) -> str:
    """
    –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å –ø–æ–º–æ—â—å—é beam search.
    length_penalty < 1.0 ‚Äî –ø–æ–æ—â—Ä—è–µ–º —á—É—Ç—å –±–æ–ª–µ–µ –¥–ª–∏–Ω–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã.
    """
    # –ö–æ–¥–∏—Ä—É–µ–º –≤–æ–ø—Ä–æ—Å
    seq = tokenizer.texts_to_sequences([clean(text)])
    seq = pad_sequences(seq, maxlen=config['max_len'], padding='post')

    # Encoder: enc_out –¥–ª—è attention + –Ω–∞—á–∞–ª—å–Ω—ã–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è h, c
    enc_out, h, c = encoder.predict(seq, verbose=0)

    # –ò–Ω–¥–µ–∫—Å—ã —Å–ø–µ—Ü—Ç–æ–∫–µ–Ω–æ–≤
    start_token = tokenizer.word_index.get('<start>', 1)
    end_token = tokenizer.word_index.get('<end>', 2)
    end_word_idx = tokenizer.word_index.get('end')
    unk_idx = tokenizer.word_index.get('<unk>')

    # –ò–Ω–¥–µ–∫—Å ‚Üí —Å–ª–æ–≤–æ
    index_word = {i: w for w, i in tokenizer.word_index.items()}

    # Beam: [(–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å_–∏–Ω–¥–µ–∫—Å–æ–≤, score, (h, c))]
    beams = [([start_token], 0.0, (h, c))]
    completed = []

    max_len = config['max_len']

    for _ in range(max_len):
        new_beams = []

        for seq_tokens, score, (st_h, st_c) in beams:
            last_idx = seq_tokens[-1]

            # –ï—Å–ª–∏ —É–∂–µ –¥–æ—à–ª–∏ –¥–æ –∫–æ–Ω—Ü–∞ –ø–æ –∏–Ω–¥–µ–∫—Å—É ‚Äî —Å—á–∏—Ç–∞–µ–º –ª—É—á –∑–∞–≤–µ—Ä—à—ë–Ω–Ω—ã–º
            if last_idx == end_token or (end_word_idx is not None and last_idx == end_word_idx):
                completed.append((seq_tokens, score))
                continue

            target = np.array([[last_idx]])
            preds, new_h, new_c = decoder.predict([target, enc_out, st_h, st_c], verbose=0)

            preds = preds[0, -1, :]

            # –ë–µ—Ä—ë–º top-k —Ç–æ–∫–µ–Ω–æ–≤
            top_k = np.argsort(preds)[-beam_width:]

            for idx in top_k:
                # –ù–µ —Ä–∞—Å—à–∏—Ä—è–µ–º <unk>-–≤–µ—Ç–∫–∏ –ø–æ –∏–Ω–¥–µ–∫—Å—É
                if unk_idx is not None and idx == unk_idx:
                    continue

                # –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –Ω—É–ª–µ–≤–æ–π —Ç–æ–∫–µ–Ω
                if idx == 0:
                    continue

                new_seq = seq_tokens + [idx]
                # –ß–µ–º –º–µ–Ω—å—à–µ score, —Ç–µ–º –ª—É—á—à–µ ‚Üí –∏—Å–ø–æ–ª—å–∑—É–µ–º -log p
                new_score = score - np.log(preds[idx] + 1e-10)
                new_beams.append((new_seq, new_score, (new_h, new_c)))

        if not new_beams:
            break

        # –û—Å—Ç–∞–≤–ª—è–µ–º top beam_width –ª—É—á–µ–π –ø–æ —Ç–µ–∫—É—â–µ–º—É score
        beams = sorted(new_beams, key=lambda x: x[1])[:beam_width]

    # –ï—Å–ª–∏ –µ—Å—Ç—å –∑–∞–≤–µ—Ä—à—ë–Ω–Ω—ã–µ –ª—É—á–∏ ‚Äî –≤—ã–±–∏—Ä–∞–µ–º –ª—É—á—à–∏–π —Å —É—á—ë—Ç–æ–º length penalty
    def norm_score(item):
        seq_tokens, score = item
        length = max(len(seq_tokens), 1)
        # –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è: –¥–µ–ª–∏–º –Ω–∞ length**length_penalty
        return score / (length ** length_penalty)

    if completed:
        best_seq = min(completed, key=lambda x: norm_score((x[0], x[1])))[0]
    else:
        # –ò–Ω–∞—á–µ –±–µ—Ä—ë–º –ª—É—á—à–∏–π –∏–∑ –Ω–µ–∑–∞–≤–µ—Ä—à—ë–Ω–Ω—ã—Ö
        if beams:
            best_seq = beams[0][0]
        else:
            best_seq = [start_token]

    # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –∏–Ω–¥–µ–∫—Å—ã –≤ —Å–ª–æ–≤–∞
    result_tokens = []
    for idx in best_seq[1:]:  # –ø—Ä–æ–ø—É—Å–∫–∞–µ–º <start>
        # –°—Ç–æ–ø –ø–æ –∏–Ω–¥–µ–∫—Å—É
        if idx == end_token or (end_word_idx is not None and idx == end_word_idx):
            break

        word = index_word.get(idx)
        if not word:
            break

        # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —è–≤–Ω—ã–π –º—É—Å–æ—Ä
        if word in ['<unk>', '<start>', '<end>']:
            continue
        if word.lower() == 'end':
            continue

        result_tokens.append(word)

    raw_text = ' '.join(result_tokens)
    return postprocess_response(raw_text)


# ================== –ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ï–ô –ò –ó–ê–ü–£–°–ö –ß–ê–¢–ê ==================

print("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π...")
encoder = load_model('encoder.keras')
decoder = load_model('decoder.keras')

with open('tokenizer.pkl', 'rb') as f:
    tokenizer = pickle.load(f)

with open('config.pkl', 'rb') as f:
    config = pickle.load(f)

print("\n" + "=" * 60)
print("ü§ñ –ß–ê–¢-–ë–û–¢ –ì–û–¢–û–í (beam search –ø–æ –¥–µ—Ñ–æ–ª—Ç—É)!")
print("=" * 60)
print("–ù–∞–ø–∏—à–∏ —á—Ç–æ-–Ω–∏–±—É–¥—å.")
print("–ö–æ–º–∞–Ω–¥–∞: '–≤—ã—Ö–æ–¥' ‚Äî –∑–∞–≤–µ—Ä—à–∏—Ç—å.")
print("=" * 60 + "\n")

while True:
    user = input("–í—ã: ").strip()

    if not user:
        continue

    if user.lower() in ['–≤—ã—Ö–æ–¥', 'exit', 'quit']:
        print("–ü–æ–∫–∞! üëã")
        break

    try:
        response = beam_search(user, encoder, decoder, tokenizer, config, beam_width=5)
        if not response:
            response = "–Ω–µ –ø–æ–Ω—è–ª"
        print(f"–ë–æ—Ç: {response}\n")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞: {e}\n")
